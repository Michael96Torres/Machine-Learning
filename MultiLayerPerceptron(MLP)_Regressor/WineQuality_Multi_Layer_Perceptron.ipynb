{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Wine Quality Report Using Multi-Layer Perceptron (MLP) Regressor**\n",
    "\n",
    "####\n",
    "\n",
    "##### The dataset that was chosen consists of various attiributes related to the components of wine quality.  The dataset was cleaned to consist of attributes which allow for linear regression anlysis.  These include attributes consisting of fixed acidity, citric acid, residual sugar, pH, sulpates, and alcohol content.  Previously we utilized ordinary least squares (OLS) and linear regression with gradient descent to gain insight into our dataset.  We decided to further expanded upon our analysis using various Multi-Layer Perceptron (MLP) models.  \n",
    "\n",
    "\n",
    "#### Link to Dataset: https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "       const  fixed acidity  citric acid  residual sugar    pH  sulphates\n",
      "0       1.0            7.4         0.00             1.9  3.51       0.56\n",
      "1       1.0            7.8         0.00             2.6  3.20       0.68\n",
      "2       1.0            7.8         0.04             2.3  3.26       0.65\n",
      "3       1.0           11.2         0.56             1.9  3.16       0.58\n",
      "4       1.0            7.4         0.00             1.9  3.51       0.56\n",
      "...     ...            ...          ...             ...   ...        ...\n",
      "1594    1.0            6.2         0.08             2.0  3.45       0.58\n",
      "1595    1.0            5.9         0.10             2.2  3.52       0.76\n",
      "1596    1.0            6.3         0.13             2.3  3.42       0.75\n",
      "1597    1.0            5.9         0.12             2.0  3.57       0.71\n",
      "1598    1.0            6.0         0.47             3.6  3.39       0.66\n",
      "\n",
      "[1599 rows x 6 columns] \n",
      "\n",
      "y:\n",
      " 0        9.4\n",
      "1        9.8\n",
      "2        9.8\n",
      "3        9.8\n",
      "4        9.4\n",
      "        ... \n",
      "1594    10.5\n",
      "1595    11.2\n",
      "1596    11.0\n",
      "1597    10.2\n",
      "1598    11.0\n",
      "Name: alcohol, Length: 1599, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read in data\n",
    "dataSet=pd.read_csv('red_wine_quality_r.csv')\n",
    "\n",
    "#Defining input X and output y values\n",
    "# df_x=dataSet.drop(['alcohol'],axis=1).drop(['r'],axis=1).values\n",
    "# df_y=dataSet['alcohol'].values\n",
    "# df_r = dataSet['r'].values\n",
    "X = dataSet.iloc[:,:-2]\n",
    "y = dataSet.iloc[:,5]\n",
    "\n",
    "#add bias term to X\n",
    "# X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "# X[:5]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# dataSet.head()\n",
    "# print(df_y)\n",
    "print(\"X:\\n\", X, '\\n')\n",
    "print(\"y:\\n\", y)\n",
    "# print(df_r)\n",
    "# print(df_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset (80% training data/20% test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Ordinary Least Squares (OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = parameter vector\n",
    "w = np.matmul(np.matmul(np.linalg.inv(np.matmul(np.array(X).transpose(), np.array(X))), X.transpose()), y)\n",
    "# print(np.array(w))\n",
    "\n",
    "# get predictions for y for training: matrix of input * w\n",
    "ols_y_train_predict = np.matmul(np.array(X_train), w)\n",
    "# print(ols_y_train_predict)\n",
    "\n",
    "# get predictions for y for testing: matrix of input * w\n",
    "ols_y_predict = np.matmul(np.array(X_test), w.transpose())\n",
    "# print(ols_y_predict)\n",
    "\n",
    "# -------------- Not from Scratch, Ignore -------------------\n",
    "# # get predictions for y for training\n",
    "# olsModel = sm.OLS(y_train, X_train)\n",
    "# olsResults = olsModel.fit()\n",
    "# ols_y_train_predict = olsResults.predict(X_train)\n",
    "\n",
    "# # get predictions for y for testing\n",
    "# olsModel2 = sm.OLS(y_test, X_test)\n",
    "# olsResults2 = olsModel2.fit()\n",
    "# ols_y_predict = olsResults2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Linear Regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for y for training\n",
    "linReg = SGDRegressor()\n",
    "linReg.fit(X_train, y_train)\n",
    "y_train_predict = linReg.predict(X_train)\n",
    "\n",
    "# get predictions for y for testing\n",
    "linReg2 = SGDRegressor()\n",
    "linReg2.fit(X_test, y_test)\n",
    "y_predict = linReg2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics: (MSE, MAE, R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics Report: Ordinary Least Square (OLS)\n",
      "w =  [ 1.44019954 -0.01995432  1.64208084  0.02530217  2.51294299  0.48462978]\n",
      "Training MSE:  0.9684926273843617\n",
      "Training MAE:  0.8180051908752247\n",
      "Training R^2:  0.10018906335335054\n",
      "Testing MSE:   1.1283089095740835\n",
      "Testing MAE:   0.8523132631768681\n",
      "Testing R^2:   0.09367546052180065\n",
      "\n",
      "Evaluation Metrics Report: Linear Regression with Gradient Descent\n",
      "w =  [0.62976828 0.06516849 0.54318199 0.02226263 2.44629949 0.60063795]\n",
      "Intercept:  [0.63136004]\n",
      "Training MSE:  0.9963743347413795\n",
      "Training MAE:  0.8180051908752247\n",
      "Training R^2:  0.10018906335335054\n",
      "w =  [0.63965788 0.12620508 0.09632115 0.07878109 2.40430098 0.39839994]\n",
      "Intercept:  [0.63136004]\n",
      "Testing MSE:   1.3027226971441634\n",
      "Testing MAE:   0.9751155781707073\n",
      "Testing R^2:   -0.04642402318942174 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation Metrics Report: Ordinary Least Square (OLS)\")\n",
    "print(\"w = \", np.array(w))\n",
    "print(\"Training MSE: \", mean_squared_error(y_train, ols_y_train_predict))\n",
    "print(\"Training MAE: \", mean_absolute_error(y_train, y_train_predict))\n",
    "print(\"Training R^2: \", r2_score(y_train, y_train_predict))\n",
    "print(\"Testing MSE:  \", mean_squared_error(y_test, ols_y_predict))\n",
    "print(\"Testing MAE:  \", mean_absolute_error(y_test, ols_y_predict))\n",
    "print(\"Testing R^2:  \", r2_score(y_test, ols_y_predict))\n",
    "\n",
    "print(\"\\nEvaluation Metrics Report: Linear Regression with Gradient Descent\")\n",
    "print(\"w = \", linReg.coef_)\n",
    "print(\"Intercept: \", linReg.intercept_)\n",
    "print(\"Training MSE: \", mean_squared_error(y_train, y_train_predict))\n",
    "print(\"Training MAE: \", mean_absolute_error(y_train, y_train_predict))\n",
    "print(\"Training R^2: \", r2_score(y_train, y_train_predict))\n",
    "print(\"w = \", linReg2.coef_)\n",
    "print(\"Intercept: \", linReg.intercept_)\n",
    "print(\"Testing MSE:  \", mean_squared_error(y_test, y_predict))\n",
    "print(\"Testing MAE:  \", mean_absolute_error(y_test, y_predict))\n",
    "print(\"Testing R^2:  \", r2_score(y_test, y_predict), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up 3 Different Multi-Layer Perceptrons (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Regressor MLP\n",
    "mlp_1 = MLPRegressor()\n",
    "\n",
    "# MLP with altered hidden layers and activation function 'tanh'\n",
    "#mlp_2 = MLPRegressor(max_iter=500, activation = 'relu')\n",
    "mlp_2 = MLPRegressor(hidden_layer_sizes=(100,100,100), activation= 'tanh', max_iter=100)\n",
    "\n",
    "#MLP changing hidden layers and activtion function 'logistic'\n",
    "mlp_3 = MLPRegressor(hidden_layer_sizes=(10,10), activation= 'logistic', max_iter=1000)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-24 {color: black;background-color: white;}#sk-container-id-24 pre{padding: 0;}#sk-container-id-24 div.sk-toggleable {background-color: white;}#sk-container-id-24 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-24 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-24 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-24 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-24 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-24 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-24 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-24 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-24 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-24 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-24 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-24 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-24 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-24 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-24 div.sk-item {position: relative;z-index: 1;}#sk-container-id-24 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-24 div.sk-item::before, #sk-container-id-24 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-24 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-24 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-24 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-24 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-24 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-24 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-24 div.sk-label-container {text-align: center;}#sk-container-id-24 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-24 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-24\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(10, 10), max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" checked><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(10, 10), max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(activation='logistic', hidden_layer_sizes=(10, 10), max_iter=1000)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_1.fit(X_train,y_train)\n",
    "mlp_2.fit(X_train,y_train)\n",
    "mlp_3.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Layer Perceptrton 1\n",
      "Training MAE:  0.7572205975134473\n",
      "Training MSE:  0.8738560127625402\n",
      "Training R^2:  0.2108335492781812\n",
      "Testing MAE:  0.8288288932731758\n",
      "Testing MSE:  1.0734035449662165\n",
      "Testing R^2:  0.1377787011067676 \n",
      "\n",
      "Multi-Layer Perceptrton 2\n",
      "Training MAE:  0.8017493305114668\n",
      "Training MSE:  0.9538405735278874\n",
      "Training R^2:  0.13860067451408076\n",
      "Testing MAE:  0.8616918430095396\n",
      "Testing MSE:  1.1249356901896244\n",
      "Testing R^2:  0.09638503010796373 \n",
      "\n",
      "Multi-Layer Perceptrton 3\n",
      "Training MAE:  0.8632034389506875\n",
      "Training MSE:  1.108667702837059\n",
      "Training R^2:  -0.0012214178305183232\n",
      "Testing MAE:  0.9120064490550451\n",
      "Testing MSE:  1.2486636040044923\n",
      "Testing R^2:  -0.0030005579675469463\n"
     ]
    }
   ],
   "source": [
    "print(\"Multi-Layer Perceptrton 1\")\n",
    "print(\"Training MAE: \", metrics.mean_absolute_error(y_train, mlp_1.predict(X_train)))\n",
    "print(\"Training MSE: \", metrics.mean_squared_error(y_train, mlp_1.predict(X_train)))\n",
    "print(\"Training R^2: \", metrics.r2_score(y_train, mlp_1.predict(X_train)))\n",
    "\n",
    "print(\"Testing MAE: \", metrics.mean_absolute_error(y_test, mlp_1.predict(X_test)))\n",
    "print(\"Testing MSE: \", metrics.mean_squared_error(y_test, mlp_1.predict(X_test)))\n",
    "print(\"Testing R^2: \", metrics.r2_score(y_test, mlp_1.predict(X_test)), \"\\n\" )\n",
    "\n",
    "\n",
    "print(\"Multi-Layer Perceptrton 2\")\n",
    "print(\"Training MAE: \", metrics.mean_absolute_error(y_train, mlp_2.predict(X_train)))\n",
    "print(\"Training MSE: \", metrics.mean_squared_error(y_train, mlp_2.predict(X_train)))\n",
    "print(\"Training R^2: \", metrics.r2_score(y_train, mlp_2.predict(X_train)))\n",
    "\n",
    "print(\"Testing MAE: \", metrics.mean_absolute_error(y_test, mlp_2.predict(X_test)))\n",
    "print(\"Testing MSE: \", metrics.mean_squared_error(y_test, mlp_2.predict(X_test)))\n",
    "print(\"Testing R^2: \", metrics.r2_score(y_test, mlp_2.predict(X_test)), \"\\n\" )\n",
    "\n",
    "print(\"Multi-Layer Perceptrton 3\")\n",
    "print(\"Training MAE: \", metrics.mean_absolute_error(y_train, mlp_3.predict(X_train)))\n",
    "print(\"Training MSE: \", metrics.mean_squared_error(y_train, mlp_3.predict(X_train)))\n",
    "print(\"Training R^2: \", metrics.r2_score(y_train, mlp_3.predict(X_train)))\n",
    "\n",
    "print(\"Testing MAE: \", metrics.mean_absolute_error(y_test, mlp_3.predict(X_test)))\n",
    "print(\"Testing MSE: \", metrics.mean_squared_error(y_test, mlp_3.predict(X_test)))\n",
    "print(\"Testing R^2: \", metrics.r2_score(y_test, mlp_3.predict(X_test)))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
